{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import copy as cp\n",
    "import os\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Source: https://github.com/eamonustc/geodistance\n",
    "import geodistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loads movement data.\n",
    "\"\"\"\n",
    "def load_data(DATAPATH_VEN,DATAPATH_MOV):\n",
    "    data_ven = pd.read_csv(DATAPATH_VEN, encoding=\"utf-8\", error_bad_lines=False, warn_bad_lines=False)\n",
    "    data_mov = pd.read_csv(DATAPATH_MOV, encoding=\"utf-8\", error_bad_lines=False, warn_bad_lines=False)\n",
    "    \n",
    "    data_ven = data_ven[(data_ven.lng != \"\\\\\\\\N\") & (data_ven.lat != \"\\\\\\\\N\")]\n",
    "    \n",
    "    data_ven.lng = data_ven.lng.astype(float)\n",
    "    data_ven.lat = data_ven.lat.astype(float)\n",
    "    \n",
    "    data_mov.checkins = data_mov.checkins.astype(int)\n",
    "    \n",
    "    #print(\"Data geladen\")\n",
    "    return data_ven, data_mov\n",
    "\n",
    "\"\"\"\n",
    "Selects venues based on location, and returns venues located within the\n",
    "specified rectangle.\n",
    "\"\"\"\n",
    "def crop_venues(data_ven, lon_min, lon_max, lat_min, lat_max):\n",
    "    data_ven = data_ven[(data_ven.lng>=lon_min) &\n",
    "                        (data_ven.lng< lon_max) &\n",
    "                        (data_ven.lat>=lat_min) &\n",
    "                        (data_ven.lat< lat_max)]\n",
    "    return data_ven.reset_index(drop=True)\n",
    "\n",
    "def select_movements_known_venues(data_ven, data_mov):\n",
    "    ids = pd.DataFrame(data_ven.id)\n",
    "    data_mov_id1 = pd.merge(ids, data_mov    , how=\"inner\", left_on=\"id\", right_on=\"venue1\").drop([\"id\"], axis=1)\n",
    "    data_mov_id2 = pd.merge(ids, data_mov_id1, how=\"inner\", left_on=\"id\", right_on=\"venue2\").drop([\"id\"], axis=1)\n",
    "    \n",
    "    return data_mov_id2.reset_index(drop=True)\n",
    "\n",
    "\"\"\"\n",
    "Selects movements based on time: month (months) and time of day (tods).\n",
    "\"\"\"\n",
    "def select_movements_time(data_mov, months=None, tods=None):\n",
    "    if tods is not None:\n",
    "        data_mov = data_mov[data_mov.period.isin(tods)]\n",
    "    \n",
    "    if months is not None:\n",
    "        data_mov = data_mov[data_mov.month.isin(months)]\n",
    "    \n",
    "    return data_mov.reset_index(drop=True)\n",
    "\n",
    "\"\"\"\n",
    "Assigns nearest station to all venues.\n",
    "\"\"\"\n",
    "def find_closest_stations(data_ven):\n",
    "    categ = [\"Train Stations\", \"Light Rail Stations\", \"Metro Stations\", \"Tram Stations\"]\n",
    "    stations = data_ven[data_ven.category.isin(categ)].drop([\"name\", \"category\"], axis=1)\n",
    "    stations_ind = np.array(stations.index)\n",
    "    \n",
    "    lv = len(data_ven)\n",
    "    ls = len(stations)\n",
    "    data_ven_the = np.array([data_ven.lat.values*np.pi/180, ]*ls).T\n",
    "    data_ven_phi = np.array([data_ven.lng.values*np.pi/180, ]*ls).T\n",
    "    stations_the = np.array([stations.lat.values*np.pi/180, ]*lv)\n",
    "    stations_phi = np.array([stations.lng.values*np.pi/180, ]*lv)\n",
    "    dthe = data_ven_the-stations_the\n",
    "    dphi = data_ven_phi-stations_phi\n",
    "    a = np.square(np.sin(dthe/2))+np.cos(data_ven_the)*np.cos(stations_the)*np.square(np.sin(dphi/2))\n",
    "    d = 6371*2*np.arcsin(np.sqrt(a))\n",
    "    mind = np.reshape(np.min(d, axis=1), (-1, 1))\n",
    "    argmind = np.argmin(d, axis=1)\n",
    "    argmind_ind = stations_ind[argmind]\n",
    "    argmind_id = pd.DataFrame(data_ven.iloc[argmind_ind]).reset_index(drop=True)[\"id\"]\n",
    "    argmind_id.columns = [\"argmind_id\"]\n",
    "    \n",
    "    data_ven_stations = cp.deepcopy(data_ven)\n",
    "    data_ven_stations[\"argmind_id\"] = argmind_id\n",
    "    data_ven_stations[\"mind\"] = mind\n",
    "    return data_ven_stations\n",
    "\n",
    "\"\"\"\n",
    "Computes the probability of being traversed by car for every trajectory\n",
    "and multiplies the number of checkins for this trajectory by this probability.\n",
    "\"\"\"\n",
    "def calc_checkins_auto(data_ven, data_mov):\n",
    "    data_ven_stations = find_closest_stations(data_ven).set_index(\"id\")\n",
    "    \n",
    "    idA = data_mov.venue1.values\n",
    "    idB = data_mov.venue2.values\n",
    "    A = data_ven_stations.loc[idA]\n",
    "    B = data_ven_stations.loc[idB]\n",
    "    idA_T = A.argmind_id.values\n",
    "    idB_T = B.argmind_id.values\n",
    "    A_T = data_ven_stations.loc[idA_T]\n",
    "    B_T = data_ven_stations.loc[idB_T]\n",
    "    \n",
    "    A_the = A.lat.values\n",
    "    A_phi = A.lng.values\n",
    "    B_the = B.lat.values\n",
    "    B_phi = B.lng.values\n",
    "    dthe = A_the-B_the\n",
    "    dphi = A_phi-B_phi\n",
    "    a = np.square(np.sin(dthe/2))+np.cos(A_the)*np.cos(B_the)*np.square(np.sin(dphi/2))\n",
    "    d = 6371*2*np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    A_T_the = A_T.lat.values\n",
    "    A_T_phi = A_T.lng.values\n",
    "    B_T_the = B_T.lat.values\n",
    "    B_T_phi = B_T.lng.values\n",
    "    dthe_T = A_T_the-B_T_the\n",
    "    dphi_T = A_T_phi-B_T_phi\n",
    "    a_T = np.square(np.sin(dthe_T/2))+np.cos(A_T_the)*np.cos(B_T_the)*np.square(np.sin(dphi_T/2))\n",
    "    d_T = 6371*2*np.arcsin(np.sqrt(a_T))\n",
    "    \n",
    "    # Los Angeles\n",
    "    '''\n",
    "    alpha = 3.6\n",
    "    u_walk  = alpha*(-0.2*d)\n",
    "    u_auto  = alpha*(-0.6-0.025*d)\n",
    "    u_train = alpha*(-0.2*(A.mind.values+B.mind.values)-0.025*d_T-1.2)\n",
    "    '''\n",
    "    # Tokyo\n",
    "    alpha=2\n",
    "    u_walk =alpha*(-0.25*d+0.2)\n",
    "    u_auto =alpha*(-1.85-0.05*d)\n",
    "    u_train=alpha*(-0.25*(A.mind.values+B.mind.values)-0.05*d_T-1.4)\n",
    "    \n",
    "    P_walk  = np.exp(u_walk)/(np.exp(u_walk)+np.exp(u_auto)+np.exp(u_train))\n",
    "    P_auto  = np.exp(u_auto)/(np.exp(u_walk)+np.exp(u_auto)+np.exp(u_train))\n",
    "    P_train = np.exp(u_train)/(np.exp(u_walk)+np.exp(u_auto)+np.exp(u_train))\n",
    "    \n",
    "    data_mov.checkins *= P_auto\n",
    "    \n",
    "    return data_mov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=20)\n",
    "\n",
    "\"\"\"\n",
    "Selects the nr movements with the most checkins.\n",
    "\"\"\"\n",
    "def select_venues_movements_most_checkins(data_ven, data_mov, nr):\n",
    "    data_mov_n = data_mov.sort_values(by=\"checkins\", ascending=False)[:nr]\n",
    "    id1_n = pd.DataFrame(data_mov_n.venue1)\n",
    "    id2_n = pd.DataFrame(data_mov_n.venue2)\n",
    "    ids_n = pd.concat([    id1_n.rename(index=str, columns={\"venue1\": \"id\"}),\n",
    "                        id2_n.rename(index=str, columns={\"venue2\": \"id\"})]).drop_duplicates().reset_index(drop=True)\n",
    "    data_ven_n = pd.merge(ids_n, data_ven)\n",
    "    \n",
    "    return data_ven_n.reset_index(drop=True), data_mov_n\n",
    "\n",
    "\"\"\"\n",
    "Clusters all remaining venues to the nr venues with the highest checkins, based\n",
    "on the minimal Euclidean distance. The origin and destination venues are\n",
    "changed to that of the nearest high-checkin venue, after which duplicates are\n",
    "summed and deleted.\n",
    "\"\"\"\n",
    "def cluster(data_ven, data_mov, nr):\n",
    "    data_ven_n, data_mov_n = select_venues_movements_most_checkins(data_ven, data_mov, nr)\n",
    "    vals_ven_pos   =   data_ven.drop([\"id\", \"name\", \"category\"], axis=1).values\n",
    "    vals_ven_pos_n = data_ven_n.drop([\"id\", \"name\", \"category\"], axis=1).values\n",
    "    ids   = pd.DataFrame(data_ven.id  )\n",
    "    ids_n = pd.DataFrame(data_ven_n.id)\n",
    "    n = len(data_ven_n)\n",
    "    N = len(data_ven)\n",
    "    \n",
    "    # Find nearest neighbour\n",
    "    lat   = np.matrix([vals_ven_pos  [:, 0], ]*n).T\n",
    "    lat_n = np.matrix([vals_ven_pos_n[:, 0], ]*N)\n",
    "    lon   = np.matrix([vals_ven_pos  [:, 1], ]*n).T\n",
    "    lon_n = np.matrix([vals_ven_pos_n[:, 1], ]*N)    \n",
    "    r_sq = np.square(lat-lat_n)+np.square(lon-lon_n)\n",
    "    r_argmin = np.ravel(np.argmin(r_sq, axis=1))\n",
    "    ids_argmin = ids_n.iloc[r_argmin].reset_index(drop=True)\n",
    "    ids_nn = pd.concat([ids, ids_argmin], axis=1)\n",
    "    ids_nn.columns = [\"id\", \"id_nn\"]\n",
    "    \n",
    "    # Add ids_nn to data_ven\n",
    "    data_ven_nn = pd.merge(ids_nn, data_ven, how=\"inner\", on=\"id\")\n",
    "    \n",
    "    # Replace venue1 and venue2 in data_mov by values from the appropriate id\n",
    "    # of the nn-table\n",
    "    data_mov_nn = pd.merge(ids_nn, data_mov   , how=\"inner\", left_on=\"id\", right_on=\"venue2\")\n",
    "    data_mov_nn = data_mov_nn.drop([\"id\", \"venue2\"], axis=1).rename(index=str, columns={\"id_nn\": \"venue2\"})\n",
    "    data_mov_nn = pd.merge(ids_nn, data_mov_nn, how=\"inner\", left_on=\"id\", right_on=\"venue1\")\n",
    "    data_mov_nn = data_mov_nn.drop([\"id\", \"venue1\"], axis=1).rename(index=str, columns={\"id_nn\": \"venue1\"})\n",
    "    \n",
    "    # Sum checkins of duplicates and drop duplicates\n",
    "    data_mov_nn[\"checkins_nn\"] = data_mov_nn.groupby([\"venue1\", \"venue2\", \"month\", \"period\"])[\"checkins\"].transform(\"sum\")\n",
    "    data_mov_nn = data_mov_nn.drop_duplicates(subset=[\"venue1\", \"venue2\", \"month\", \"period\"]).drop([\"checkins\"], axis=1)\n",
    "    data_mov_nn = data_mov_nn[data_mov_nn.venue1 != data_mov_nn.venue2]\n",
    "    \n",
    "    return data_ven_nn, data_mov_nn\n",
    "\n",
    "\n",
    "def compute_num_clusters(auto_num_clusters,custom_num_clusters,avg_distance,num_lanes,max_density):\n",
    "    num_clusters = -1\n",
    "    if(not(auto_num_clusters)):\n",
    "        num_clusters = custom_num_clusters\n",
    "    else:\n",
    "        num_clusters = int(total_movements / (avg_distance*num_lanes*max_density/2))\n",
    "    return num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_datasets(num_movements,num_clusters,datapath_ven,datapath_mov):\n",
    "    daily_traffic_amount = num_movements \n",
    "\n",
    "    # Load and process data\n",
    "    data_ven, data_mov = load_data(datapath_ven,datapath_mov)\n",
    "    data_ven = crop_venues(data_ven, 139.704515, 139.861661, 35.594325, 35.726745)\n",
    "    data_mov = select_movements_known_venues(data_ven, data_mov)\n",
    "    data_mov = select_movements_time(data_mov, tods=[\"AFTERNOON\"]) \n",
    "\n",
    "    # Scale\n",
    "\n",
    "    data_mov = calc_checkins_auto(data_ven,data_mov)\n",
    "\n",
    "    # Cluster data\n",
    "    data_ven_clustered, data_mov_clustered = cluster(data_ven, data_mov, num_clusters)\n",
    "\n",
    "    # Drop original venue ids, drop duplicates of new ids\n",
    "    data_ven_clustered = data_ven_clustered.drop('id',axis=1)\n",
    "    data_ven_new = data_ven_clustered.drop_duplicates(subset=\"id_nn\", keep='first', inplace=False).dropna()\n",
    "\n",
    "    # Take average checkins per timestep (morning/afternoon/evening/night, so 6 hours)\n",
    "    # Timestep is currently useless since we already select on afternoon only\n",
    "    data_mov_clustered['checkins_nn'] = data_mov_clustered.groupby(['venue1', 'venue2'])['checkins_nn'].transform('sum')\n",
    "    data_mov_clustered['checkins_count_month'] = data_mov_clustered.groupby(['venue1', 'venue2'])['checkins_nn'].transform('count')\n",
    "    data_mov_clustered['checkins_count_month_time'] = data_mov_clustered.groupby(['venue1', 'venue2', 'month'])['checkins_nn'].transform('count')\n",
    "    data_mov_clustered = data_mov_clustered.drop_duplicates(subset=(\"venue1\",\"venue2\"), keep='first', inplace=False).dropna()\n",
    "    data_mov_clustered['checkins_avg_per_timestep'] = data_mov_clustered['checkins_nn'] / data_mov_clustered['checkins_count_month'] / data_mov_clustered['checkins_count_month_time']\n",
    "    data_mov_clustered['checkins_percentage'] = data_mov_clustered['checkins_avg_per_timestep'] / np.sum(data_mov_clustered['checkins_avg_per_timestep'].values) * daily_traffic_amount\n",
    "    # Above not really percentage, cause it already multiplies\n",
    "\n",
    "    # Drop movement intermediate columns\n",
    "    data_mov_clustered = data_mov_clustered.drop(['checkins_count_month','checkins_count_month_time','checkins_avg_per_timestep'],axis=1)\n",
    "\n",
    "    # Drop duplicates\n",
    "    data_mov_new = data_mov_clustered.drop_duplicates(subset=(\"venue1\",\"venue2\"), keep='first', inplace=False).dropna()\n",
    "\n",
    "    return data_ven_new,data_mov_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph and Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph with lat/lon of most visited stuff\n",
    "\n",
    "\n",
    "def build_graph(shapefile_path,save_graph,num_lanes,max_density,max_speed):\n",
    "\n",
    "    # Load road network\n",
    "    # Based on code from:\n",
    "    # https://gis.stackexchange.com/questions/256955/how-to-load-a-weighed-shapefile-in-networkx\n",
    "    X=nx.read_shp(shapefile_path).to_undirected()\n",
    "\n",
    "    coords = {}\n",
    "    pos = {k: v for k,v in enumerate(X.nodes())}\n",
    "    G=nx.Graph()\n",
    "    for nodenum,coord_pair in pos.items():\n",
    "        lat = coord_pair[1]\n",
    "        lon = coord_pair[0]\n",
    "        G.add_node(\"intersection_\"+str(nodenum), lng=lon, lat=lat, node_type='road_intersection')\n",
    "        coords[\"intersection_\" + str(nodenum)] = (lat,lon)\n",
    "\n",
    "\n",
    "    l=[set(x) for x in X.edges()] #To speed things up in case of large objects\n",
    "    edg=[tuple(k for k,v in pos.items() if v in sl) for sl in l] #Map the G.edges start and endpoints onto pos\n",
    "\n",
    "    for i in range(0,len(edg)):\n",
    "        tup = edg[i]\n",
    "        x = tup[0]\n",
    "        y = tup[1]\n",
    "        x = \"intersection_\" + str(x)\n",
    "        y = \"intersection_\" + str(y)\n",
    "        new_tup = (x,y)\n",
    "        edg[i] = new_tup\n",
    "\n",
    "    j = 0\n",
    "    for edge in edg:\n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "        data1 = G.nodes[node1]\n",
    "        data2 = G.nodes[node2]\n",
    "        dist,br = geodistance.distanceHaversine(data1['lat'],data1['lng'],data2['lat'],data2['lng']) # Distance in km\n",
    "        fee1 = np.random.uniform(low=1,high=10) # Initialize fees randomly\n",
    "        G.add_edge(node1,\n",
    "                   node2,\n",
    "                   distance=dist,\n",
    "                   fee=fee1,\n",
    "                   total_weight=dist+fee1,\n",
    "                   num_lanes=num_lanes,\n",
    "                   key=\"road_\"+str(j),\n",
    "                   occupancy=0,\n",
    "                   travel_time=0,\n",
    "                   max_velocity=max_speed,\n",
    "                   max_density=max_density\n",
    "                  )\n",
    "        j += 1\n",
    "\n",
    "\n",
    "\n",
    "    # Add nodes for venues\n",
    "\n",
    "    for idex,row in data_ven_new.iterrows():\n",
    "        node_id = row[0] # We'll need this to identify the nodes\n",
    "        node_lat = float(row[2])\n",
    "        node_lon = float(row[3])\n",
    "        G.add_node('venue_'+str(node_id), lng=node_lon, lat=node_lat, key=node_id, node_type='venue') # can't export if pos is defined; do manually in gephi\n",
    "        # Connect venue to nearest neighbor\n",
    "\n",
    "        lowest_nodenum = -1\n",
    "        lowest_dist = float('inf')\n",
    "        for nodenum,data in coords.items():\n",
    "            target_lat = data[0]\n",
    "            target_lon = data[1]\n",
    "            dist,br = geodistance.distanceHaversine(node_lat,node_lon,target_lat,target_lon)\n",
    "            if(dist < lowest_dist):\n",
    "                lowest_nodenum = nodenum\n",
    "                lowest_dist = dist\n",
    "        fee1 = np.random.uniform(low=1,high=10)\n",
    "        G.add_edge(\"venue_\"+str(node_id),\n",
    "                   lowest_nodenum,\n",
    "                   distance=lowest_dist,\n",
    "                   fee=fee1,\n",
    "                   total_weight=dist+fee1,\n",
    "                   num_lanes=num_lanes,\n",
    "                   key=\"road_\"+str(j),\n",
    "                   occupancy=0,\n",
    "                   travel_time=0,\n",
    "                   max_velocity=max_speed,\n",
    "                   max_density=999999999999\n",
    "                  )\n",
    "        j += 1\n",
    "\n",
    "\n",
    "\n",
    "    # Export for gephi; display there using geo-layout\n",
    "    if(save_graph):\n",
    "        nx.write_gexf(G, \"graph_shapefile_taipeigen.gexf\")\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function\n",
    "\n",
    "def obj(individual,beta,G,data_mov,min_speed):\n",
    "    # Step 0: Set fee values for all edges using parameters of the solution\n",
    "    i = 0\n",
    "    \n",
    "    for edge_data in G.edges.data():\n",
    "        a = edge_data[0]\n",
    "        b = edge_data[1]\n",
    "        d = edge_data[2]\n",
    "        key = d['key']\n",
    "        dist = d['distance']\n",
    "        fee = d['fee']\n",
    "        total_weight = beta*dist+individual[i]\n",
    "        # when a, b and key are the same, add_edge modifies existing\n",
    "        G.add_edge(a,b,key=key,fee=individual[i],\n",
    "                   total_weight=total_weight,occupancy=0) # Reset occupancy to prevent explosion\n",
    "        i += 1\n",
    "    \n",
    "\n",
    "    \n",
    "    # Step 1: for all clustered origin-destination pairs, find the shortest path using distance+fees\n",
    "    \n",
    "    # For each route, maintain tuple of (time,checkins) for both conditions\n",
    "    travel_times = []\n",
    "    travel_times_checkins = []\n",
    "    # For each route, remember path taken, may be tough on memory, but can't do it during\n",
    "    # other loops due to road-focus. Need occupancy from all routes before computing travel\n",
    "    # time of specific route.\n",
    "    paths = []\n",
    "\n",
    "    for idex,row in data_mov_new.iterrows():\n",
    "        # Iterate over origin-destination pairs\n",
    "        \n",
    "        origin = \"venue_\" + row[0]\n",
    "        destination = \"venue_\" + row[1]\n",
    "        num_checkins = row[5]\n",
    "        \n",
    "        # Find shortest path\n",
    "        path = nx.dijkstra_path(G,origin,destination,weight='total_weight')\n",
    "        paths.append(path)\n",
    "        \n",
    "        last_node = None\n",
    "\n",
    "        # Step 2: use total amount of check-ins for those pairs to compute total amount of cars on each road segment of the path\n",
    "        # Update occupancy by num_checkins\n",
    "        \n",
    "        \n",
    "        for node_name in path:\n",
    "            if(last_node != None):\n",
    "                d = G.get_edge_data(last_node,node_name)\n",
    "                key = d['key']\n",
    "                occupancy = d['occupancy']\n",
    "                max_dens = d['max_density']\n",
    "                num_lanes = d['num_lanes']\n",
    "                dist = d['distance']\n",
    "                if(((occupancy+num_checkins)/dist) > (max_dens*num_lanes)):\n",
    "                    G.add_edge(last_node,node_name,key=key,occupancy=occupancy+num_checkins,fee=999999999999)\n",
    "                else:\n",
    "                    G.add_edge(last_node,node_name,key=key,occupancy=occupancy+num_checkins)\n",
    "            last_node = node_name\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "    # Step 3: use traffic formulas to compute the average time spent on each road segment\n",
    "    total_time_all_roads = 0\n",
    "    i = 0\n",
    "    for edge_data in G.edges.data():\n",
    "        a = edge_data[0]\n",
    "        b = edge_data[1]\n",
    "        d = edge_data[2]\n",
    "        num_lanes = d['num_lanes']\n",
    "        key = d['key']\n",
    "        occ = d['occupancy']\n",
    "        dist = d['distance']\n",
    "        max_vel = d['max_velocity']\n",
    "        max_dens = d['max_density'] * num_lanes\n",
    "        opt_dens = max_dens/2\n",
    "        max_f = max_vel * opt_dens\n",
    "        \n",
    "        \n",
    "        if(occ>0):\n",
    "            dens = occ/dist         \n",
    "            fp = (max_f/(opt_dens**2)) * max(0,dens*(2*(max_dens/2)-dens))\n",
    "            avg_speed = min(max_vel,fp/dens)        \n",
    "            average_travel_time = dist/max(avg_speed,min_speed)\n",
    "           \n",
    "        else:\n",
    "            average_travel_time = 0\n",
    "            \n",
    "        # Step 4: multiply time by amount of cars to get a total time spent\n",
    "        total_travel_time = average_travel_time * occ\n",
    "        G.add_edge(a,b,key=key,travel_time=average_travel_time)\n",
    "        \n",
    "        # Step 5: sum for all roads\n",
    "        \n",
    "        total_time_all_roads += total_travel_time\n",
    "        i += 1\n",
    "\n",
    "    # Step 6: Compute distribution of times\n",
    "        \n",
    "\n",
    "    i = 0\n",
    "    for idex,row in data_mov_new.iterrows():\n",
    "        # Iterate over origin-destination pairs\n",
    "        \n",
    "        origin = \"venue_\" + row[0]\n",
    "        destination = \"venue_\" + row[1]\n",
    "        num_checkins = row[5]\n",
    "        path = paths[i]\n",
    "        i += 1\n",
    "        \n",
    "        current_time = 0\n",
    "        last_node = None\n",
    "        for node_name in path:\n",
    "            if(last_node != None):\n",
    "                d = G.get_edge_data(last_node,node_name)\n",
    "                travel_time = d['travel_time']\n",
    "                current_time += travel_time                \n",
    "            last_node = node_name\n",
    "        travel_times.append(current_time)\n",
    "        travel_times_checkins.append(num_checkins) # Only need this once\n",
    "\n",
    "    \n",
    "    x_cost = []\n",
    "    i = 0\n",
    "    for time in travel_times:\n",
    "        for j in range(0,int(travel_times_checkins[i])):\n",
    "            x_cost.append(time)\n",
    "        i+=1\n",
    "    return sum(x_cost)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard GA\n",
    "\n",
    "\n",
    "def run_GA(G,data_mov_new,iteration_budget,population_size,mutation_rate,sigma,beta,min_speed):\n",
    "\n",
    "    genome_length = len(G.edges)\n",
    "\n",
    "    # Global variables\n",
    "\n",
    "    best_fitness_per_iteration = []\n",
    "    best_individual = np.zeros(genome_length)\n",
    "    best_fitness = float('inf') # Objective is to minimize\n",
    "\n",
    "    # Initialize population randomly\n",
    "\n",
    "    population = np.random.uniform(0,1,size=(population_size,genome_length)) # Rows are individuals, columns are fees\n",
    "\n",
    "    # Start main loop\n",
    "    for iter in range(1,iteration_budget+1):\n",
    "        working_population = np.zeros((2*population_size,genome_length+1))\n",
    "\n",
    "        # Recombination\n",
    "\n",
    "        i = 0\n",
    "        while(i<population_size):\n",
    "            parent1 = population[i,:]\n",
    "            parent2 = population[i+1,:]\n",
    "            crossover_point = np.random.randint(1,genome_length-1)\n",
    "            offspring1 = np.concatenate([parent1[0:crossover_point],parent2[crossover_point:genome_length]])\n",
    "            offspring2 = np.concatenate([parent2[0:crossover_point],parent1[crossover_point:genome_length]])\n",
    "            working_population[i,0:genome_length] = offspring1\n",
    "            working_population[i+1,0:genome_length] = offspring2\n",
    "            working_population[i+population_size,0:genome_length] = parent1\n",
    "            working_population[i+1+population_size,0:genome_length] = parent2\n",
    "            i += 2\n",
    "\n",
    "\n",
    "        # Mutation\n",
    "\n",
    "        for i in range(0,population_size):\n",
    "            for j in range(0,genome_length):\n",
    "                if(np.random.uniform() < mutation_rate):\n",
    "                    working_population[i,j] = max(0,working_population[i,j] + np.random.normal(loc=0,scale=sigma))\n",
    "\n",
    "        # Store best individual and its fitness\n",
    "\n",
    "        for i in range(0,2*population_size):\n",
    "            a = datetime.datetime.now()\n",
    "            working_population[i,genome_length] = obj(working_population[i,0:genome_length],beta,\n",
    "                                                      G,data_mov_new,min_speed) # Not +1 cause 0 indexing\n",
    "            b = datetime.datetime.now()\n",
    "            delta = b - a\n",
    "\n",
    "        ranking = working_population[working_population[:,genome_length].argsort()]\n",
    "        local_top_individual = ranking[0,0:genome_length]\n",
    "        local_top_fitness = ranking[0,genome_length]\n",
    "        best_fitness_per_iteration.append(local_top_fitness)\n",
    "        if(local_top_fitness < best_fitness):\n",
    "            best_fitness = local_top_fitness\n",
    "            best_individual = local_top_individual\n",
    "\n",
    "        # Select and update population\n",
    "\n",
    "        population = ranking[0:population_size,0:genome_length]\n",
    "\n",
    "    # Uncomment to plot fitness improvement\n",
    "\n",
    "    #plt.plot(best_fitness_per_iteration)\n",
    "    #plt.ylabel('Fitness')\n",
    "    #plt.xlabel('Iteration')\n",
    "    #plt.show()\n",
    "    return best_individual, best_fitness_per_iteration\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of objective function, used to save output graph with a list of parameters \n",
    "\n",
    "# Define objective function\n",
    "\n",
    "def obj_eval(individual,best_fitness,beta,G,data_mov,experiment_name,histogram_bins,sigma,num_clusters,min_speed):\n",
    "    # Step 0: Set fee values for all edges using parameters of the solution\n",
    "    i = 0\n",
    "    \n",
    "    for edge_data in G.edges.data():\n",
    "        a = edge_data[0]\n",
    "        b = edge_data[1]\n",
    "        d = edge_data[2]\n",
    "        key = d['key']\n",
    "        dist = d['distance']\n",
    "        fee = d['fee']\n",
    "        total_weight = beta*dist+individual[i]\n",
    "        # when a, b and key are the same, add_edge modifies existing\n",
    "        G.add_edge(a,b,key=key,fee=individual[i],\n",
    "                   total_weight=total_weight,occupancy=0) # Reset occupancy to prevent explosion\n",
    "        i += 1\n",
    "    \n",
    "    G2 = G.copy()\n",
    "    \n",
    "    # Step 1: for all clustered origin-destination pairs, find the shortest path using distance+fees\n",
    "    \n",
    "    # For each route, maintain tuple of (time,checkins) for both conditions\n",
    "    travel_times = []\n",
    "    travel_times_nocost = []\n",
    "    travel_times_checkins = []\n",
    "    # For each route, remember path taken, may be tough on memory, but can't do it during\n",
    "    # other loops due to road-focus. Need occupancy from all routes before computing travel\n",
    "    # time of specific route.\n",
    "    paths = []\n",
    "    paths_nocost = []\n",
    "    \n",
    "    \n",
    "    for idex,row in data_mov_new.iterrows():\n",
    "        # Iterate over origin-destination pairs\n",
    "        \n",
    "        origin = \"venue_\" + row[0]\n",
    "        destination = \"venue_\" + row[1]\n",
    "        num_checkins = row[5]\n",
    "        \n",
    "        # Find shortest path\n",
    "        path = nx.dijkstra_path(G,origin,destination,weight='total_weight')\n",
    "        paths.append(path)\n",
    "        path_nocost = nx.dijkstra_path(G2,origin,destination,weight='distance')\n",
    "        paths_nocost.append(path_nocost)\n",
    "        \n",
    "        last_node = None\n",
    "\n",
    "        # Step 2: use total amount of check-ins for those pairs to compute total amount of cars on each road segment of the path\n",
    "        # Update occupancy by num_checkins\n",
    "        \n",
    "        \n",
    "        \n",
    "        for node_name in path:\n",
    "            if(last_node != None):\n",
    "                d = G.get_edge_data(last_node,node_name)\n",
    "                key = d['key']\n",
    "                occupancy = d['occupancy']\n",
    "                max_dens = d['max_density']\n",
    "                num_lanes = d['num_lanes']\n",
    "                dist = d['distance']\n",
    "                if(((occupancy+num_checkins)/dist) > (max_dens*num_lanes)):\n",
    "                    G.add_edge(last_node,node_name,key=key,occupancy=occupancy+num_checkins,fee=999999999999)\n",
    "                else:\n",
    "                    G.add_edge(last_node,node_name,key=key,occupancy=occupancy+num_checkins)\n",
    "            last_node = node_name\n",
    "            \n",
    "        last_node = None\n",
    "        for node_name in path_nocost:\n",
    "            if(last_node != None):\n",
    "                d = G2.get_edge_data(last_node,node_name)\n",
    "                key = d['key']\n",
    "                occupancy = d['occupancy']\n",
    "                max_dens = d['max_density']\n",
    "                num_lanes = d['num_lanes']\n",
    "                dist = d['distance']\n",
    "                if(((occupancy+num_checkins)/dist) > (max_dens*num_lanes)):\n",
    "                    G2.add_edge(last_node,node_name,key=key,occupancy=occupancy+num_checkins,fee=999999999999)\n",
    "                else:\n",
    "                    G2.add_edge(last_node,node_name,key=key,occupancy=occupancy+num_checkins)\n",
    "            last_node = node_name\n",
    "    \n",
    "\n",
    "    \n",
    "    # Step 3: use traffic formulas to compute the average time spent on each road segment\n",
    "    total_time_all_roads = 0\n",
    "    i = 0\n",
    "    for edge_data in G.edges.data():\n",
    "        a = edge_data[0]\n",
    "        b = edge_data[1]\n",
    "        d = edge_data[2]\n",
    "        num_lanes = d['num_lanes']\n",
    "        key = d['key']\n",
    "        occ = d['occupancy']\n",
    "        dist = d['distance']\n",
    "        max_vel = d['max_velocity']\n",
    "        max_dens = d['max_density'] * num_lanes\n",
    "        opt_dens = max_dens/2\n",
    "        max_f = max_vel * opt_dens\n",
    "        \n",
    "        \n",
    "        if(occ>0):\n",
    "            dens = occ/dist         \n",
    "            fp = (max_f/(opt_dens**2)) * max(0,dens*(2*(max_dens/2)-dens))\n",
    "            avg_speed = min(max_vel,fp/dens)        \n",
    "            average_travel_time = dist/max(avg_speed,min_speed)\n",
    "           \n",
    "        else:\n",
    "            average_travel_time = 0\n",
    "            \n",
    "        # Step 4: multiply time by amount of cars to get a total time spent\n",
    "        total_travel_time = average_travel_time * occ\n",
    "        G.add_edge(a,b,key=key,travel_time=average_travel_time)\n",
    "        \n",
    "        # Step 5: sum for all roads\n",
    "        \n",
    "        total_time_all_roads += total_travel_time\n",
    "        i += 1\n",
    "\n",
    "    # Step 6: Compute distribution of times\n",
    "        \n",
    "\n",
    "    i = 0\n",
    "    # With cost\n",
    "    for idex,row in data_mov_new.iterrows():\n",
    "        # Iterate over origin-destination pairs\n",
    "        \n",
    "        origin = \"venue_\" + row[0]\n",
    "        destination = \"venue_\" + row[1]\n",
    "        num_checkins = row[5]\n",
    "        path = paths[i]\n",
    "        i += 1\n",
    "        \n",
    "        current_time = 0\n",
    "        last_node = None\n",
    "        for node_name in path:\n",
    "            if(last_node != None):\n",
    "                d = G.get_edge_data(last_node,node_name)\n",
    "                travel_time = d['travel_time']\n",
    "                current_time += travel_time                \n",
    "            last_node = node_name\n",
    "        travel_times.append(current_time)\n",
    "        travel_times_checkins.append(num_checkins) # Only need this once\n",
    "\n",
    "        \n",
    "        \n",
    "    # Repeat steps 3-6 Without cost for comparison\n",
    "    \n",
    "    # Step 3: use traffic formulas to compute the average time spent on each road segment\n",
    "    total_time_all_roads = 0\n",
    "    i = 0\n",
    "    for edge_data in G2.edges.data():\n",
    "        a = edge_data[0]\n",
    "        b = edge_data[1]\n",
    "        d = edge_data[2]\n",
    "        num_lanes = d['num_lanes']\n",
    "        key = d['key']\n",
    "        occ = d['occupancy']\n",
    "        dist = d['distance']\n",
    "        max_vel = d['max_velocity']\n",
    "        max_dens = d['max_density'] * num_lanes\n",
    "        opt_dens = max_dens/2\n",
    "        max_f = max_vel * opt_dens\n",
    "        \n",
    "        \n",
    "        if(occ>0):\n",
    "            dens = occ/dist         \n",
    "            fp = (max_f/(opt_dens**2)) * max(0,dens*(2*(max_dens/2)-dens))\n",
    "            avg_speed = min(max_vel,fp/dens)        \n",
    "            average_travel_time = dist/max(avg_speed,min_speed) \n",
    "        else:\n",
    "            average_travel_time = 0\n",
    "            \n",
    "  \n",
    "        # Step 4: multiply time by amount of cars to get a total time spent\n",
    "        total_travel_time = average_travel_time * occ\n",
    "        G2.add_edge(a,b,key=key,travel_time=average_travel_time)\n",
    "        \n",
    "        # Step 5: sum for all roads\n",
    "        total_time_all_roads += total_travel_time\n",
    "        i += 1\n",
    "\n",
    "        \n",
    "    # Step 6: Compute distribution of times\n",
    "              \n",
    "    i = 0\n",
    "    # Without cost\n",
    "    for idex,row in data_mov_new.iterrows():\n",
    "        # Iterate over origin-destination pairs\n",
    "        \n",
    "        origin = \"venue_\" + row[0]\n",
    "        destination = \"venue_\" + row[1]\n",
    "        num_checkins = row[5]\n",
    "        path = paths_nocost[i]\n",
    "        i += 1\n",
    "        \n",
    "        current_time = 0\n",
    "        last_node = None\n",
    "        for node_name in path:\n",
    "            if(last_node != None):\n",
    "                d = G2.get_edge_data(last_node,node_name)\n",
    "                travel_time = d['travel_time']\n",
    "                current_time += travel_time                \n",
    "            last_node = node_name\n",
    "        travel_times_nocost.append(current_time)\n",
    "\n",
    "    \n",
    "    # Compute statistical measures\n",
    "    \n",
    "    measures = [min(travel_times),max(travel_times),np.mean(travel_times),np.median(travel_times),np.std(travel_times)]\n",
    "    measures_nocost = [min(travel_times_nocost),max(travel_times_nocost),np.mean(travel_times_nocost),\n",
    "                       np.median(travel_times_nocost),np.std(travel_times_nocost)]\n",
    "    \n",
    "    # Histogram preparation, since hist doesn't seem to work with explicitly specifying frequencies (checkins)\n",
    "    x_cost = []\n",
    "    x_nocost = []\n",
    "    i = 0\n",
    "    for time in travel_times:\n",
    "        for j in range(0,int(travel_times_checkins[i])):\n",
    "            x_cost.append(time)\n",
    "        i+=1\n",
    "    i = 0\n",
    "    for time in travel_times_nocost:\n",
    "        for j in range(0,int(travel_times_checkins[i])):\n",
    "            x_nocost.append(time)\n",
    "        i+=1\n",
    "        \n",
    "    # Step 7: Save stuff\n",
    "\n",
    "    # Check if directory exists\n",
    "    if(not(os.path.isdir(\"results/\" + experiment_name))):\n",
    "        os.mkdir(\"results/\" + experiment_name)\n",
    "    \n",
    "    # Find number\n",
    "    it = 0\n",
    "    for file in os.listdir(\"results/\" + experiment_name + \"/\"):\n",
    "        if file.endswith(\".gexf\"):\n",
    "            it += 1\n",
    "    \n",
    "    # Save graph \n",
    "    for e in G.edges.data():\n",
    "        d = e[2]\n",
    "        d['occupancy'] = int(d['occupancy'])\n",
    "        d['fee'] = float(d['fee'])\n",
    "        d['total_weight'] = float(d['total_weight'])\n",
    "    nx.write_gexf(G, \"results/\" + experiment_name + \"/run\" + str(it) + \"_graph.gexf\")\n",
    "    \n",
    "    # Save algorithm progress figure\n",
    "    x = range(1,iteration_budget+1)\n",
    "    default_fitness = obj(np.zeros(len(G.edges)),beta,G,data_mov_new,min_speed)\n",
    "    random_fitness = 0\n",
    "    for i in range(0,5): # Hardcoded as it's not central, can be parameterised\n",
    "        r = abs(np.random.randn(len(G.edges))) * sigma\n",
    "        random_fitness += obj(r,beta,G,data_mov_new,min_speed)\n",
    "    random_fitness = random_fitness / (i+1)\n",
    "    default_fitness_improvement = abs((default_fitness - best_fitness) / default_fitness * 100)\n",
    "    random_fitness_improvement = abs((random_fitness - best_fitness) / random_fitness * 100)\n",
    "    d = [default_fitness for i in range(1,iteration_budget+1)]\n",
    "    r = [random_fitness for i in range(1,iteration_budget+1)]\n",
    "    \n",
    "    plt.plot(x,best_fitness_per_iteration)\n",
    "    plt.ylabel('Fitness')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.plot(x,d)\n",
    "    plt.plot(x,r)\n",
    "    plt.legend(['GA','No external costs','Random external costs'])\n",
    "    plt.savefig(\"results/\" + experiment_name + \"/run\" + str(it) + \"_optimisation_progress.png\")\n",
    "    plt.clf()\n",
    "    \n",
    "    # Save time distribution figures\n",
    "    num_bins = histogram_bins\n",
    "    \n",
    "    max_val_x = max(max(x_cost),max(x_nocost))\n",
    "    n, bins, patches = plt.hist(x_cost, num_bins, facecolor='blue', alpha=0.5)\n",
    "    plt.xlabel(\"Time spent on the road in hours, using external costs\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlim(left=0,right=max_val_x)\n",
    "    plt.savefig(\"results/\" + experiment_name + \"/run\" + str(it) + \"_hist.png\")\n",
    "    stored_ylim = plt.gca().get_ylim()\n",
    "    plt.clf()\n",
    "    \n",
    "    n, bins, patches = plt.hist(x_nocost, bins=bins, facecolor='blue', alpha=0.5)\n",
    "    plt.xlabel(\"Time spent on the road in hours, without external costs\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlim(left=0,right=max_val_x)\n",
    "    plt.ylim(stored_ylim)\n",
    "    plt.savefig(\"results/\" + experiment_name + \"/run\" + str(it) + \"_hist_nocost.png\")\n",
    "    plt.clf()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Roads hist\n",
    "    edge_times = []\n",
    "    for edge_data in G.edges.data():\n",
    "        d = edge_data[2]\n",
    "        edge_times.append(d['travel_time'])\n",
    "        \n",
    "    \n",
    "    n, bins, patches = plt.hist(edge_times, num_bins, facecolor='blue', alpha=0.5)\n",
    "    plt.xlabel(\"Time spent on roads\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.savefig(\"results/\" + experiment_name + \"/run\" + str(it) + \"_hist_roads.png\")\n",
    "    plt.clf()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Save parameter settings\n",
    "    \n",
    "    if(it==0):\n",
    "        # Write header if file doesn't exist yet\n",
    "        with open(\"results/\" + experiment_name + \"/settings_\" + experiment_name + \".csv\",'w') as fp:\n",
    "            s = \"run_num,default_fitness,random_fitness,best_fitness,default_fitness_improvement,\\\n",
    "            random_fitness_improvement,[best_fitness_per_iteration],iteration_budget,mutation_rate,sigma,beta,\\\n",
    "            [min_time,max_time,mean_time,median_time,std_time],\\\n",
    "            [min_time_nocost,max_time_nocost,mean_time_nocost,median_time_nocost,std_time_nocost],num_clusters\\n\"\n",
    "            fp.write(s)\n",
    "    \n",
    "    with open(\"results/\" + experiment_name + \"/settings_\" + experiment_name + \".csv\",'a') as fp:\n",
    "        param_string = str(it) + \",\"+ str(default_fitness) + \",\" + str(random_fitness) + \",\" + \\\n",
    "                        str(best_fitness) + \",\" + str(default_fitness_improvement) + \",\" + \\\n",
    "                        str(random_fitness_improvement) + \",\" + str(best_fitness_per_iteration) + \",\" + \\\n",
    "                        str(iteration_budget) + \",\" + str(mutation_rate) + \",\" + str(sigma) + \",\" + \\\n",
    "                        str(beta) + \",\" + str(measures) + \",\" + str(measures_nocost) + \",\" + str(num_clusters) + \"\\n\"\n",
    "        fp.write(param_string)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controller function for experiments\n",
    "\n",
    "# Experiment parameters (default in comments)\n",
    "experiment_name = \"newobj_movements_25000\"\n",
    "\n",
    "shapefile_path = 'shapefiles/groads/tokyo_smallest.shp' # 'shapefiles/groads/tokyo_smallest.shp'\n",
    "num_runs = 1 # 5\n",
    "histogram_bins = 10 # 15\n",
    "save_graph = False # False\n",
    "\n",
    "# Data handling parameters\n",
    "total_movements = 25000 # 25000\n",
    "avg_distance = 1.0948129267976516 # Computed beforehand\n",
    "datapath = \"foursquare/\" # \"foursquare/\"\n",
    "auto_num_clusters = False # True\n",
    "custom_num_clusters = 38 # 100\n",
    "datapath_ven = datapath + \"venues/Tokyo_venue_info.csv\" # \"venues/Tokyo_venue_info.csv\"\n",
    "datapath_mov = datapath + \"movements/Tokyo_movements.csv\" # \"movements/Tokyo_movements.csv\"\n",
    "\n",
    "# Graph parameters\n",
    "\n",
    "num_lanes = 2*2\n",
    "max_density = 300\n",
    "max_speed = 100\n",
    "\n",
    "\n",
    "# GA parameters\n",
    "iteration_budget = 30 # 30\n",
    "population_size = 20 # 20\n",
    "mutation_rate = 0.2 # 0.2\n",
    "sigma = 4 # 4\n",
    "beta = 2 # 2\n",
    "\n",
    "# Obj parameters\n",
    "\n",
    "min_speed = 15\n",
    "\n",
    "\n",
    "\n",
    "# Compute num_clusters\n",
    "\n",
    "num_clusters = compute_num_clusters(auto_num_clusters,custom_num_clusters,avg_distance,num_lanes,max_density)\n",
    "print(\"Number of clusters: \" + str(num_clusters))\n",
    "print(\"Projected runtime for \" + str(num_runs) + \" runs: \" + str(int(num_clusters/200*15000/60*num_runs)) + \" minutes.\")\n",
    "print(\"Started running at: \" + str(time.time()))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Load movement data\n",
    "\n",
    "print(\"Loading and processing movement data...\")\n",
    "data_ven_new,data_mov_new = load_datasets(total_movements,num_clusters,datapath_ven,datapath_mov)\n",
    "print(\"...done.\")\n",
    "\n",
    "\n",
    "# Load graph data\n",
    "\n",
    "print(\"Loading spatial data and building graph...\")\n",
    "G = build_graph(shapefile_path,save_graph,num_lanes,max_density,max_speed)\n",
    "print(\"...done.\")\n",
    "\n",
    "\n",
    "\n",
    "# Run experiments\n",
    "\n",
    "print(\"Starting experiments...\\n\")\n",
    "start_time = time.time()\n",
    "for i in range(0,num_runs):\n",
    "    print(\"Running experiment \" + str(i+1) + \"/\" + str(num_runs))\n",
    "    iteration_start_time = time.time()\n",
    "    G = build_graph(shapefile_path,False,num_lanes,max_density,max_speed) # Since it takes little computational power, just to prevent carry-over effects\n",
    "    best_individual, best_fitness_per_iteration = run_GA(G,data_mov_new,iteration_budget,population_size,mutation_rate,sigma,beta,min_speed)\n",
    "    obj_eval(best_individual,best_fitness_per_iteration[-1],beta,G,data_mov_new,experiment_name,histogram_bins,sigma,num_clusters,min_speed)\n",
    "    iteration_end_time = time.time()\n",
    "    print(\"...done. Experiment runtime: \" + str(iteration_end_time - iteration_start_time))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"\\nExperiments concluded. Total runtime: \" + str(end_time - start_time))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for running experiments with random total_movements\n",
    "\n",
    "# Experiment parameters (default in comments)\n",
    "experiment_name = \"newgen_total_movement_opt_autocluster\"\n",
    "\n",
    "shapefile_path = 'shapefiles/groads/tokyo_smallest.shp' # 'shapefiles/groads/tokyo_smallest.shp'\n",
    "num_runs = 20 # 5\n",
    "histogram_bins = 15 # 15\n",
    "save_graph = False # False\n",
    "\n",
    "# Data handling parameters\n",
    "max_total_movements = 150000 # 12000000 (12 million)\n",
    "min_total_movements = 25000\n",
    "num_clusters = 100 # 100\n",
    "avg_distance = 1.0948129267976516 # Computed beforehand\n",
    "datapath = \"foursquare/\" # \"foursquare/\"\n",
    "auto_num_clusters = True # True\n",
    "custom_num_clusters = 100 # 100\n",
    "datapath_ven = datapath + \"venues/Tokyo_venue_info.csv\" # \"venues/Tokyo_venue_info.csv\"\n",
    "datapath_mov = datapath + \"movements/Tokyo_movements.csv\" # \"movements/Tokyo_movements.csv\"\n",
    "\n",
    "# Graph parameters\n",
    "\n",
    "num_lanes = 2*2\n",
    "max_density = 300\n",
    "max_speed = 100\n",
    "\n",
    "\n",
    "# GA parameters\n",
    "iteration_budget = 30 # 30\n",
    "population_size = 20 # 20\n",
    "mutation_rate = 0.2 # 0.2\n",
    "sigma = 4 # 4\n",
    "beta = 2 # 2\n",
    "\n",
    "# Obj parameters\n",
    "\n",
    "min_speed = 15\n",
    "\n",
    "\n",
    "\n",
    "# Run experiments\n",
    "\n",
    "#data_string = \"\"\n",
    "\n",
    "print(\"Starting experiments...\\n\")\n",
    "start_time = time.time()\n",
    "for i in range(0,num_runs):\n",
    "    print(\"Running experiment \" + str(i+1) + \"/\" + str(num_runs))\n",
    "    iteration_start_time = time.time()\n",
    "    total_movements = int(np.random.uniform(min_total_movements,max_total_movements))\n",
    "    print(\"Total movements: \" + str(total_movements))\n",
    "    num_clusters = compute_num_clusters(auto_num_clusters,custom_num_clusters,avg_distance,num_lanes,max_density)\n",
    "    print(\"Number of clusters: \" + str(num_clusters))\n",
    "    data_ven_new,data_mov_new = load_datasets(total_movements,num_clusters,datapath_ven,datapath_mov)\n",
    "    G = build_graph(shapefile_path,False,num_lanes,max_density,max_speed)\n",
    "    default_fitness = obj(np.zeros(len(G.edges)),beta,G,data_mov_new,min_speed)\n",
    "    best_individual, best_fitness_per_iteration = run_GA(G,data_mov_new,iteration_budget,population_size,mutation_rate,sigma,beta,min_speed)\n",
    "    improvement = abs((best_fitness_per_iteration[-1] - default_fitness) / default_fitness)\n",
    "    obj_eval(best_individual,best_fitness_per_iteration[-1],beta,G,data_mov_new,experiment_name,histogram_bins,sigma,num_clusters,min_speed)\n",
    "    #data_string = (data_string + str(total_movements) + \",\" + str(improvement) + \"\\n\")\n",
    "    data_string = (str(total_movements) + \",\" + str(improvement) + \"\\n\")\n",
    "    with open(\"results/newgen_random_total_movements_autocluster.csv\",'a') as fp:\n",
    "        fp.write(data_string)\n",
    "    iteration_end_time = time.time()\n",
    "    print(\"...done. Experiment runtime: \" + str(iteration_end_time - iteration_start_time))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"\\nExperiments concluded and results saved. Total runtime: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of total_movements and improvement percentage\n",
    "\n",
    "d = pd.read_csv(\"results/newgen_random_total_movements_autocluster.csv\",header=None)\n",
    "x = d.as_matrix()[:,0]\n",
    "y = d.as_matrix()[:,1] * 100\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.xlim(0,180000)\n",
    "plt.xlabel(\"Total amount of movements\")\n",
    "plt.ylabel(\"Improvement percentage\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
